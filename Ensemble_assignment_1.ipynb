{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Techniques\n"
      ],
      "metadata": {
        "id": "Y0SkaIVY0YV-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8Pgtm4v0U0-"
      },
      "outputs": [],
      "source": [
        "# Q1. What is an ensemble technique in machine learning?\n",
        "# An ensemble technique in machine learning is a method that combines multiple individual models (often called base models or weak learners) to improve predictive performance.\n",
        "# Instead of relying on a single model, ensemble techniques leverage the diversity and collective wisdom of multiple models to make more accurate predictions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2. Why are ensemble techniques used in machine learning?\n",
        "# Ensemble techniques are used for several reasons:\n",
        "\n",
        "# Improved Accuracy: By combining multiple models, ensemble techniques can reduce bias and variance, leading to better overall performance.\n",
        "# Robustness: Ensembles can be more robust to noisy data and outliers compared to individual models.\n",
        "# Generalization: Ensembles can generalize well to new, unseen data.\n",
        "# Versatility: They can be applied to a wide range of machine learning tasks and algorithms."
      ],
      "metadata": {
        "id": "V9rqOtUy0lk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3. What is bagging?\n",
        "# Bagging (Bootstrap Aggregating) is an ensemble technique where multiple base models are trained independently on different subsets of the training data.\n",
        "# Each subset is sampled with replacement (bootstrap samples), and the final prediction is often made by averaging the predictions of all models (for regression) or taking a majority vote (for classification)."
      ],
      "metadata": {
        "id": "chL_O0KF0lqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4. What is boosting?\n",
        "# Boosting is another ensemble technique where base models (typically weak learners) are trained sequentially, with each subsequent model focusing on improving the prediction errors made by the previous models.\n",
        "# In boosting, the models are trained iteratively, and each model pays more attention to instances that were incorrectly predicted by the previous models."
      ],
      "metadata": {
        "id": "4hTtMB4I0luf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5. What are the benefits of using ensemble techniques?\n",
        "# The benefits of ensemble techniques include:\n",
        "\n",
        "# Improved Accuracy: Ensembles often outperform individual models by reducing bias and variance.\n",
        "# Robustness: They are less sensitive to overfitting and noise in the data.\n",
        "# Versatility: They can combine different types of models and algorithms.\n",
        "# Generalization: Ensembles can generalize well to new data.\n",
        "# Reduced Risk: Ensembles are less likely to be affected by outliers or erroneous data points."
      ],
      "metadata": {
        "id": "zTf_8f0t0lyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6. Are ensemble techniques always better than individual models?\n",
        "# Not necessarily. While ensemble techniques often yield better performance than individual models, there are scenarios where:\n",
        "\n",
        "# The data is too clean or simple, and the additional complexity of an ensemble doesn't provide significant benefits.\n",
        "# Computational resources or time constraints might make training and maintaining an ensemble impractical.\n",
        "# Interpretability may be more important than predictive power, and ensembles can be harder to interpret than individual models."
      ],
      "metadata": {
        "id": "7y8Z3so50l1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7. How is the confidence interval calculated using bootstrap?\n",
        "# To calculate the confidence interval using bootstrap, follow these steps:\n",
        "\n",
        "# Sample with Replacement: Generate multiple bootstrap samples by randomly sampling with replacement from the original dataset.\n",
        "# Calculate Statistic: Compute the statistic of interest (e.g., mean, median) for each bootstrap sample.\n",
        "# Calculate Bootstrap Distribution: Create a distribution of the statistic based on the results from step 2.\n",
        "# Determine Confidence Interval: Use percentiles of the bootstrap distribution to determine the confidence interval. For a 95% confidence interval, typically use the 2.5th and 97.5th percentiles of the bootstrap distribution."
      ],
      "metadata": {
        "id": "5XKbyR690l4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q8. How does bootstrap work and what are the steps involved?\n",
        "# Bootstrap is a resampling technique used to estimate statistics about a population by sampling with replacement from the original dataset. Here are the steps involved:\n",
        "\n",
        "# Sample with Replacement: Randomly select samples (of the same size as the original dataset) from the original dataset, allowing the same sample to appear multiple times (with replacement).\n",
        "# Compute Statistic: Compute the statistic of interest (e.g., mean, median, standard deviation) for each bootstrap sample.\n",
        "# Repeat: Repeat steps 1 and 2 a large number of times (typically thousands of times) to create a distribution of the statistic.\n",
        "# Calculate Confidence Interval: Use the percentiles of the distribution (e.g., 2.5th and 97.5th percentiles for a 95% confidence interval) to estimate the confidence interval of the statistic."
      ],
      "metadata": {
        "id": "EzVJUnkW0l7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q9. Estimating the 95% confidence interval using bootstrap for the researcher's problem\n",
        "# Steps to estimate the 95% confidence interval:\n",
        "\n",
        "# Generate Bootstrap Samples: Randomly sample 50 values with replacement from the original sample data.\n",
        "# Compute Bootstrap Mean: Calculate the mean height for each bootstrap sample.\n",
        "# Calculate Bootstrap Standard Error\n",
        "# Determine Confidence Interval\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Given data\n",
        "sample_mean = 15\n",
        "sample_std = 2\n",
        "n = 50\n",
        "z_critical = 1.96  # for 95% confidence interval\n",
        "\n",
        "# Step 1: Generate bootstrap samples\n",
        "np.random.seed(0)  # for reproducibility\n",
        "bootstrap_means = []\n",
        "for _ in range(10000):\n",
        "    bootstrap_sample = np.random.choice(np.random.normal(sample_mean, sample_std, n), size=n, replace=True)\n",
        "    bootstrap_means.append(np.mean(bootstrap_sample))\n",
        "\n",
        "# Step 2: Calculate standard error of bootstrap distribution\n",
        "bootstrap_std_error = np.std(bootstrap_means)\n",
        "\n",
        "# Step 3: Calculate 95% confidence interval\n",
        "lower_bound = sample_mean - z_critical * bootstrap_std_error\n",
        "upper_bound = sample_mean + z_critical * bootstrap_std_error\n",
        "\n",
        "print(f\"95% Confidence Interval for the population mean height: ({lower_bound:.2f}, {upper_bound:.2f}) meters\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySXixx4F0l-l",
        "outputId": "c004900c-0740-4841-ca9b-5bb74530e68b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95% Confidence Interval for the population mean height: (14.21, 15.79) meters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OTjsI1dy0mBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LCNa7_xZ0mEE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}